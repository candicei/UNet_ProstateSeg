{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadcb01e-68a1-4ab4-be33-db9b01399d4b",
   "metadata": {},
   "source": [
    "# UNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d5553-9d9d-46ad-82ff-608257cdd08e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d350b-a0f2-4726-9516-0bdfe7a89cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom data-sets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "%matplotlib inline \n",
    "from typing import List, Callable, Tuple\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc390e-f1c6-45f7-badb-44b188cce789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def makeDir(bpath):\n",
    "    \"\"\"Makes a new, non-overwriting folder (makes a new folder name if name already exists), returns path of folder\"\"\"\n",
    "    path=bpath\n",
    "    pathMade=False\n",
    "    numPath=1\n",
    "    while not(pathMade):\n",
    "        try:\n",
    "            os.makedirs(path, exist_ok=False)\n",
    "            pathMade = True\n",
    "        except:\n",
    "            path=bpath+str(numPath)\n",
    "            numPath+=1\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243cec4-887e-4900-b3dc-755e01c8528e",
   "metadata": {},
   "source": [
    "### Dataset Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f50b25-09bb-4e48-abce-2573fd000a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Methods and Object Functions\"\"\"\n",
    "# Creating method to import custom datasets\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, im_dir, mask_dirs, roi_type, transform=None):        \n",
    "        self.im_dir = im_dir\n",
    "        self.im_masks_dir = mask_dirs\n",
    "        self.transform = transform\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.long\n",
    "        self.roi_type = roi_type\n",
    "        \n",
    "    def __len__(self): # rerturning the number of samples we have\n",
    "        return len(self.im_masks_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        # making sure everything is forward slash\n",
    "        file2Load_mask = \"/\".join(self.im_masks_dir[idx].split(\"\\\\\"))\n",
    "        n_pat  = re.compile(\"(?<=/P)\\d*(?!>_S)\").findall(file2Load_mask)[0] # getting string of number - patient number \n",
    "        if n_pat==\"\":\n",
    "            n_pat=re.compile(\"(?<=/P)\\d*(?!>_S)\").findall(file2Load_mask)[1] # getting string of number - patient number \n",
    "        if int(n_pat) <100: # logic to get the correct file name since <100 pat number becomes 0##\n",
    "            n_pat = '0' + n_pat\n",
    "        n_slice = re.compile(\"(?<=_S)\\d*(?!>_)\").findall(file2Load_mask)[0] # getting string of number - slice number\n",
    "        file2Load_im = \"/\".join(self.im_dir.split(\"\\\\\")) + '/P' + n_pat + '_cmplx.mat' # getting file name of T2 decay data\n",
    "                \n",
    "        # Loading data\n",
    "        mask = loadmat(file2Load_mask)[self.roi_type]\n",
    "        image = (loadmat(file2Load_im)['cmplx'][:,:,int(n_slice),:]) # grabbing correct T2 decay data\n",
    "        name = \"P\"+n_pat+\"_S\"+n_slice\n",
    "        \n",
    "        # Augmentations/Tranformations\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            \n",
    "        # transofrming data to torch datatype\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.type(self.inputs_dtype) # permuting necessary for correct input order\n",
    "            mask = mask.type(self.targets_dtype)\n",
    "        else:\n",
    "            image = torch.from_numpy(image).type(self.inputs_dtype).permute(2,0,1) # permuting necessary for correct input order\n",
    "            mask = torch.from_numpy(mask).type(self.targets_dtype)\n",
    "        \n",
    "        return image, mask, name\n",
    "\n",
    "    \n",
    "# splitData(splitTrain,fileNames) \n",
    "#  - Mostly random spilt of files for training / validation\n",
    "#  input:\n",
    "#    splitTrain - the percentage to of the data used for training in decimal form\n",
    "#    fileNames  - the names of the files used for training / validation\n",
    "#  returns:\n",
    "#    folder_train - the names of the files used for training, split by splitTrain percentage\n",
    "#    folder_valid - the names of the files used for validation\n",
    "def splitData(split_train,fileNames):\n",
    "    folder_train = glob.glob(fileNames + '/*.mat')\n",
    "    folder_valid = []\n",
    "    numTotal = np.size(folder_train)\n",
    "    numTrain = int(np.round(numTotal *split_train))\n",
    "    numValid = np.size(fileNames) - numTrain\n",
    "\n",
    "    # flag for quitting the while loop\n",
    "    breakFlag = 0\n",
    "    while ~breakFlag:\n",
    "        # obtaining a random seed number - since patients should be lumped together for vaildation or training set\n",
    "        n = random.randint(0,np.size(folder_train)-1)\n",
    "\n",
    "        # parsing string to get patient number\n",
    "        pnum = re.compile(\"(?<=/P)\\d*(?!>_S)\").findall(\"/\".join(folder_train[n].split(\"\\\\\")))[0]\n",
    "\n",
    "        indices = [i for i, elem in enumerate(folder_train) if 'P'+pnum in elem]\n",
    "        # print(indices[::-1])\n",
    "\n",
    "        # If at the end of the split, we'll need to randomly select slices from a patient. This logic below does that\n",
    "        if (np.size(folder_train) - np.size(indices)) <= numTrain:\n",
    "            breakFlag = 1        \n",
    "            # calculating how many more training datasets need to moved to the validation set,\n",
    "            # nlastMove <= indices\n",
    "            nlastMove =  np.size(folder_train)  - numTrain\n",
    "            indices = sorted(random.sample(indices,nlastMove)) #gathering a random subset of indices\n",
    "\n",
    "        # Appending files to the tempList\n",
    "        for i in indices[::-1]:\n",
    "            folder_valid.append(folder_train.pop(folder_train.index(folder_train[i])))\n",
    "\n",
    "        if breakFlag:\n",
    "#             print(numTotal, numTrain, numValid, np.size(folder_train), np.size(tempList))\n",
    "            return folder_train, folder_valid\n",
    "\n",
    "#         print(numTotal, numTrain, numValid, np.size(folder_train), np.size(tempList), np.size(indices))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6cf47-3c37-4b3f-b774-b0d02af3f2c2",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888ad64-d98b-48b0-b1b5-4d14d011a7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_transform = A.Compose(\n",
    "#     [\n",
    "#         A.Resize(240,240),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.001, scale_limit=0.01, rotate_limit=2, border_mode=2, p=0.5),\n",
    "#         A.Normalize(mean=0.5, std=0.25),\n",
    "#         A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "#         A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, approximate=False, p=0.5),\n",
    "#         A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "#         A.Resize(240,240),\n",
    "        A.ShiftScaleRotate(shift_limit=0.001, scale_limit=0.01, rotate_limit=2, border_mode=2, p=0.5),\n",
    "#         A.Normalize(mean=0.5, std=0.25),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.1, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "        A.ElasticTransform(alpha=0.5, sigma=10, alpha_affine=10, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, approximate=False, p=0.5),\n",
    "        A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, interpolation=1, border_mode=2, value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ada5c9-c15c-441b-b6a3-3f4b1d7d34e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448960b-2b1d-40a6-b676-70b17e88d3dd",
   "metadata": {},
   "source": [
    "### Creating the Neural Network (UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ffd94-9ccd-4bc1-b8af-a914152f7e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    model = UNET_V2(params[\"in_channels\"],params[\"out_channels\"])\n",
    "    model = model.to(params[\"device\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaaf95c-bcf2-492d-9847-0415ac4b3480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network \n",
    "# - July 9, 2021\n",
    "class UNET_V2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNET_V2, self).__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(in_channels, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)         \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)        \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, out_channels, kernel_size=1, padding=0)     \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)         \n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)         \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)         \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)        \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d((2, 2))     \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_channels+out_channels, out_channels)     \n",
    "    \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)         \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)         \n",
    "        self.relu = nn.ReLU()     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c6761-7ae5-4aec-b99b-f81765f9d51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "class UNET_V1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNET_V1, self).__init__()       \n",
    "        \n",
    "        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "\n",
    "        self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "\n",
    "        upconv3 = self.upconv3(conv3)\n",
    "\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        return upconv1\n",
    "    \n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec796a-514d-4af1-adf5-e6fc547fb591",
   "metadata": {},
   "source": [
    "### Training/Validating Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2394297-6f41-44af-8285-3f4b7e8f12e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params, logs=None):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    epoch_loss= [] # for plotting training\n",
    "    for i, (images, target, name) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"], non_blocking=True)\n",
    "        target = target.to(params[\"device\"], non_blocking=True)\n",
    "        target = target.float() # need to cast to float\n",
    "#         output = model(images).squeeze(1)\n",
    "        output = model(images)\n",
    "    \n",
    "        loss = criterion(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stream.set_description(\"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "        \n",
    "        epoch_loss = loss\n",
    "        epoch_acc = ((output.squeeze()>=0.5)==target).sum()/(target.size(0)*target.size(1)*target.size(2))\n",
    "        logs['Training loss'] = epoch_loss.item()\n",
    "        logs['Training accuracy'] = epoch_acc.item()\n",
    "\n",
    "    return logs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fb7d7-071d-4b90-a36c-9fb31b538480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, logs=None):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    epoch_loss= [] # for plotting validation\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, name) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.float()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            stream.set_description(\"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "            \n",
    "            epoch_loss = loss\n",
    "            epoch_acc = ((output.squeeze()>=0.5)==target).sum()/(target.size(0)*target.size(1)*target.size(2))\n",
    "            logs['Validation loss'] = epoch_loss.item()\n",
    "            logs['Validation accuracy'] = epoch_acc.item()\n",
    "         \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a5e5a-9a14-44e8-8e80-ab1407e5109e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_dataset, val_dataset, params, savePath=None):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "#     criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n",
    "    criterion = FocalLoss(gamma=params[\"gamma\"],alpha=params[\"alpha\"]).to(params[\"device\"])\n",
    "#     criterion = WeightedFocalLoss(gamma=params[\"gamma\"],alpha=params[\"alpha\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    \n",
    "    # for plotting\n",
    "    if params[\"plotting\"]:\n",
    "#         loss_train =  [] # for plotting training\n",
    "#         loss_valids =  [] # for plotting validation\n",
    "#         fig,ax = plt.subplots(1,2);\n",
    "        liveloss = PlotLosses(outputs=[MatplotlibPlot(figpath =savePath)])\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        \n",
    "        logs={}\n",
    "        # logic to provide the correct axis to the plotter\n",
    "        if params[\"plotting\"]:\n",
    "            logs=train(train_loader, model, criterion, optimizer, epoch, params, logs)\n",
    "            logs=validate(val_loader, model, criterion, epoch, params, logs)\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "        else:\n",
    "            train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "            validate(val_loader, model, criterion, epoch, params)\n",
    "        \n",
    "    if params[\"plotting\"]:\n",
    "        return model, liveloss\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f33af-8fee-45c5-8011-848df610f26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, params, test_dataset, batch_size):\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, mask in test_loader:\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images)\n",
    "            probabilities = torch.sigmoid(output.squeeze(1))\n",
    "            predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "            predicted_masks = predicted_masks.cpu().numpy()\n",
    "            for predicted_mask in predicted_masks:\n",
    "                predictions.append(predicted_mask)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a2c51-257c-4677-88f0-178877546240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateModel(predIms,gtIms):\n",
    "    \"\"\"Calculates 3 evaluations: pixel accuracy, jaccard's index, and dice coefficient of a prediction and mask\"\"\"\n",
    "    pIm =predIms.squeeze()==1 # prediction image\n",
    "    mIm = gtIms.squeeze()==1 # mask image\n",
    "    \n",
    "    numPix = np.shape(mIm)[0]*np.shape(mIm)[1] # total number of pixels in an image\n",
    "    tp = (pIm*mIm).sum() # true positive\n",
    "    tn = (~pIm*~mIm).sum() # true negative\n",
    "    fn = (~pIm*mIm).sum() # false negatives\n",
    "    fp = (pIm*~mIm).sum() # false positives\n",
    "    \n",
    "    # Pixel Accuracy    \n",
    "    PA = (tp+tn)/numPix # pixel accuracy     \n",
    "    # Jaccard's Index (Intersection over Union, IoU)\n",
    "    JI = tp/(tp+fn+fp)\n",
    "    # Dice Coefficient\n",
    "    DC = 2*tp/(2*tp+fn+fp)\n",
    "    \n",
    "    return PA,JI,DC\n",
    "    \n",
    "\n",
    "def evaluateModelAndPredict(model, params, test_dataset, batch_size):\n",
    "    \"\"\"Calculates various metrics to evaluate the model based on predictions and ground truth\n",
    "       in addition to providing the predictions\"\"\"\n",
    "    # getting the data loader for the test set\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # getting evaluation metrics\n",
    "    PA = []  # Pixel Accuracy\n",
    "    JI = []  # Jaccard's Index (Intersection over Union, IoU)\n",
    "    DC = []  # Dice Coefficient\n",
    "    evaluation = dict() # to store evaluation metrics\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, mask, name in test_loader:\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images)\n",
    "            probabilities = torch.sigmoid(output.squeeze(1))\n",
    "            predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "            predicted_masks = predicted_masks.cpu().numpy()\n",
    "            for predicted_mask in predicted_masks:\n",
    "                predictions.append(predicted_mask)\n",
    "            \n",
    "            tempPA, tempJI, tempDC = evaluateModel(predicted_masks,mask.cpu().numpy())\n",
    "            PA.append(tempPA)\n",
    "            JI.append(tempJI)\n",
    "            DC.append(tempDC)\n",
    "    \n",
    "    evaluation[\"Pixel Accuracy\"] = PA\n",
    "    evaluation[\"Jaccard Index\"] = JI\n",
    "    evaluation[\"Dice Coefficient\"] = DC\n",
    "    \n",
    "    \n",
    "    return predictions, evaluation\n",
    "\n",
    "\n",
    "\n",
    "def evaluateModelAndPredict_andPrint(model, params, test_dataset, batch_size, mainPath, kernelSize=0):\n",
    "    \"\"\"Calculates various metrics to evaluate the model based on predictions and ground truth\n",
    "       in addition to providing the predictions\"\"\"\n",
    "    # getting the data loader for the test set\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # getting evaluation metrics\n",
    "    PA = []  # Pixel Accuracy\n",
    "    JI = []  # Jaccard's Index (Intersection over Union, IoU)\n",
    "    DC = []  # Dice Coefficient\n",
    "    evaluation = dict() # to store evaluation metrics\n",
    "    \n",
    "    \n",
    "    ### Making directory to save to\n",
    "    outputImDir = makeDir(mainPath +'/'+ 'OutputImages_k'+str(kernelSize))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, mask, name in test_loader:\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images)\n",
    "            probabilities = torch.sigmoid(output.squeeze(1))\n",
    "            predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "            predicted_masks = predicted_masks.cpu().numpy()\n",
    "            for predicted_mask in predicted_masks:\n",
    "                predictions.append(predicted_mask)\n",
    "            \n",
    "            tempPA, tempJI, tempDC = evaluateModel(predicted_masks,mask.cpu().numpy())\n",
    "            PA.append(tempPA)\n",
    "            JI.append(tempJI)\n",
    "            DC.append(tempDC)\n",
    "            \n",
    "            # output image\n",
    "            kernel = np.ones((kernelSize,kernelSize), np.uint8)\n",
    "            img_mod = cv2.dilate(cv2.erode(predicted_mask, kernel, iterations=1),kernel,iterations=1)\n",
    "            cv2.imwrite(outputImDir +\"/\" +name[0] + \".png\", img_mod * 255) # needs to be a 0 and 255 image\n",
    "    \n",
    "    evaluation[\"Pixel Accuracy\"] = PA\n",
    "    evaluation[\"Jaccard Index\"] = JI\n",
    "    evaluation[\"Dice Coefficient\"] = DC\n",
    "    \n",
    "    return predictions, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f28c3-38f2-4b6f-a97d-a363bdcdb4c5",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9baa0-b76a-43d5-b361-990709226e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "#     def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.reduction = reduction\n",
    "#         self.weight = torch.Tensor([weight,1-weight]) #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "    \n",
    "#         print(self.weight)\n",
    "#         ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "#         return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8377a5-b90b-4b4a-bc37-87e5cceb8fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha])\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        if inputs.dim()>2:\n",
    "            inputs = inputs.view(inputs.size(0),inputs.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            inputs = inputs.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            inputs = inputs.contiguous().view(-1,inputs.size(2))   # N,H*W,C => N*H*W,C\n",
    "        targets = targets.view(-1,1)\n",
    "        \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68799a-ae7f-41f2-850c-aa0c293b47e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=1, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target): \n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        \n",
    "        logpt = F.log_softmax(input, dim=0)\n",
    "        if(np.shape(input)[1]>1):\n",
    "            logpt = logpt.gather(1,target.cpu().long())\n",
    "            \n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.cpu().long().data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba229d38-b27f-4090-ae4f-1cec879d7009",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f75db-1a0e-4d49-bd4a-d02e1de1836f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"For Plotting the image batches\"\"\"\n",
    "\n",
    "def display_image_grid_loader(dataloader_obj):\n",
    "    \"\"\"Fuction to display images from a dataloader object as a grid\"\"\"\n",
    "    echonum = 2 # number 4 is usually the brightest -> around 8ms, though selected 2\n",
    "    \n",
    "    imgs, masks, name = next(iter(dataloader_obj)) # next iteration \n",
    "    gridImgs = torchvision.utils.make_grid(imgs,nrow=4, normalize=True,scale_each=True)\n",
    "#     print(np.shape(imgs), np.shape(masks))\n",
    "\n",
    "    masks = masks.reshape(np.shape(masks)[0],1,np.shape(masks)[1],np.shape(masks)[2])\n",
    "    gridMasks = torchvision.utils.make_grid(masks,nrow=4)\n",
    "#     print(np.shape(imgs), np.shape(masks))\n",
    "\n",
    "    f,ax = plt.subplots(2,figsize=(10,10))\n",
    "#     f,ax = plt.subplots(2)\n",
    "    ax[0].imshow(gridImgs[echonum,:,:], interpolation=None)\n",
    "    ax[1].imshow(gridMasks[0,:,:], interpolation=None)\n",
    "    ax[0].set_axis_off()\n",
    "    ax[1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def display_image_grid_dataset(dataset_obj, numIms, predicted_masks=None):\n",
    "    \"\"\"Function to display images from a (custom)dataset object as a grid\"\"\"\n",
    "    echonum = 4 # number 4 is usually the brightest -> around 8ms\n",
    "    \n",
    "    rows = numIms\n",
    "    cols = 3 if predicted_masks else 2\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols,figsize=(2*6,numIms*3))\n",
    "    \n",
    "    for i, batch in enumerate(dataset_obj, start=0):\n",
    "        image, mask = batch\n",
    "        image = torch.div(image[echonum,:,:],torch.max(image[echonum,:,:])) # image is normalized\n",
    "        mask = mask[:,:]\n",
    "        \n",
    "        ax[i,0].imshow(image, cmap='gray', interpolation=None) #,cmap='gray'\n",
    "        ax[i,1].imshow(mask, interpolation=Noned)\n",
    "\n",
    "\n",
    "        ax[i,0].set_ylabel(name)\n",
    "        ax[i,0].xaxis.set_ticks([])\n",
    "        ax[i,0].yaxis.set_ticks([])\n",
    "        ax[i,1].set_axis_off()\n",
    "\n",
    "        if predicted_masks:\n",
    "            predicted_mask = predicted_masks[i]\n",
    "            ax[i,2].imshow(predicted_mask, interpolation=None)\n",
    "            ax[i,2].set_axis_off()\n",
    "        \n",
    "        # break if reached number of images to print\n",
    "        if (i+1 == numIms): break\n",
    "    \n",
    "    ax[0,0].set_title(\"Image, Slice: \"+str(echonum))\n",
    "    ax[0,1].set_title(\"Ground truth mask\")   \n",
    "    if predicted_masks:\n",
    "        ax[0,2].set_title(\"Predicted mask\") \n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    \"\"\"Visualizing augmentations performed on dataset\"\"\"\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    figure, ax = plt.subplots(nrows=samples, ncols=3, figsize=(10, 24))\n",
    "    for i in range(samples):\n",
    "        image, mask, name = dataset[idx]\n",
    "        ax[i, 0].imshow(image[4,:,:], interpolation=None)\n",
    "        ax[i, 2].imshow(image[2,:,:], interpolation=None)\n",
    "        ax[i, 1].imshow(mask, interpolation=None)\n",
    "        ax[i, 0].set_title(\"Augmented image\")\n",
    "        ax[i, 1].set_title(\"Augmented mask\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "        ax[i, 2].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0fa61-7630-402b-ac54-1b44a9ccdef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f6461-2405-4bdf-8231-1622a4dd6cee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Begining the Modelling [User Input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a82ab-563f-490f-835d-82c66401b59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Initializing to CUDA (gpu framework)\"\"\"\n",
    "# CUDA for PyTorch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5640dc-4d61-4fbc-af61-b1a4d3c26f70",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b524762-f3a5-4fd3-8cb0-112530787274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=params[\"batch_size\"],\n",
    "#         shuffle=True,\n",
    "#         num_workers=params[\"num_workers\"],\n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "\n",
    "# T_im, T_mask = next(iter(train_loader))\n",
    "# output=model(T_im)\n",
    "# print(np.shape(output.squeeze()))\n",
    "# out=output.squeeze().permute(1,0).detach().numpy()\n",
    "\n",
    "# print(np.shape(out))\n",
    "# print(np.shape(T_mask))\n",
    "# plt.imshow(out)\n",
    "# plt.figure()\n",
    "# plt.imshow(T_mask.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e09f9-975d-4bb0-aaee-a2a7141dd08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Getting timestamp\"\"\"\n",
    "now = datetime.now() # current time\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "params = {\n",
    "    \"Timestamp\": timestamp,\n",
    "    \"model\": \"UNET_V2\",\n",
    "    \"segType\": \"PZ\", #or \"PZ\"\n",
    "    \"in_channels\": 64,\n",
    "    \"out_channels\": 1,\n",
    "    \"device\": device,\n",
    "    \"lr\": 0.00001,\n",
    "    \"batch_size\": 10,\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 30,\n",
    "    \"gamma\": 3,\n",
    "    \"alpha\": 0.15,\n",
    "    \"transform\": None, # None or \"train_transform\"\n",
    "    \"plotting\": 1,\n",
    "}\n",
    "# output saving information\n",
    "folder_saveoutputs = \"C:/Users/candi/Documents/Research/1 LWI Project/1 Data/7 Model Outputs/\" #need \"/\" after\n",
    "modelName = 'UNET_cpu_' + params[\"segType\"] + '_test'\n",
    "# Folder location of matlab data\n",
    "folder_data = 'C:/Users/candi/Documents/Research/1 LWI Project/1 Data/6 Datasets For ML/3 T2 Decay'\n",
    "folder_mask_PZ= 'C:/Users/candi/Documents/Research/1 LWI Project/1 Data/6 Datasets For ML/PZ'\n",
    "folder_mask_full =  'C:/Users/candi/Documents/Research/1 LWI Project/1 Data/6 Datasets For ML/Full'\n",
    "folder_test_full = 'C:/Users/candi/Documents/Research/1 LWI Project/1 Data/6 Datasets For ML/Full_TEST'\n",
    "folder_test_PZ = 'C:/Users/candi/Documents/Research/1 LWI Project/1 Data/6 Datasets For ML/PZ_TEST'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63189e7-ebf2-4008-b7ec-13f751de7436",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading and Splitting Training/Valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c85498-2b9e-4c41-ac1a-75ece94f9f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if params[\"segType\"]=='full':\n",
    "    folder_trainNvalid = folder_mask_full\n",
    "    folder_test = folder_test_full\n",
    "    roiFileStrName = 'roi_full'\n",
    "elif params[\"segType\"]=='PZ':\n",
    "    folder_trainNvalid = folder_mask_PZ\n",
    "    folder_test = folder_test_PZ\n",
    "    roiFileStrName = 'roi_subzone'\n",
    "\n",
    "# Separating Training, Validation, and Test set [20% of original already split]\n",
    "# & other training parameters\n",
    "split_train = 0.80\n",
    "split_valid = 0.20\n",
    "\n",
    "# Splitting Training / Testing / Valid\n",
    "# folder_mask_train, folder_mask_valid = splitData(split_train,folder_mask) # for PZ data\n",
    "folder_mask_train, folder_mask_valid = splitData(split_train,folder_trainNvalid)\n",
    "\n",
    "# Creating the dataloader for training / validation\n",
    "if params[\"transform\"] is not None:\n",
    "    train_dataset = CustomDataset(folder_data, folder_mask_train, roi_type = roiFileStrName, transform=eval(params[\"transform\"]))\n",
    "    valid_dataset = CustomDataset(folder_data, folder_mask_valid, roiFileStrName, transform=eval(params[\"transform\"]))\n",
    "else:\n",
    "    train_dataset = CustomDataset(folder_data, folder_mask_train, roi_type = roiFileStrName, transform=(params[\"transform\"]))\n",
    "    valid_dataset = CustomDataset(folder_data, folder_mask_valid, roiFileStrName, transform=(params[\"transform\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1352e2-ffe9-49c2-a815-db860dcef6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performing Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cf32d-67e6-405d-8329-b0d1b39e6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_saveoutputs+'/'+timestamp+\"_\"+modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f01c9e-0ad6-44e6-91ef-2df76d49008e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "mainPath = makeDir(folder_saveoutputs+'/'+timestamp+\"_\"+modelName)\n",
    "\n",
    "t = time.time() # running a timer\n",
    "model = create_model(params)\n",
    "if params[\"plotting\"]:\n",
    "    model, lossplot = train_and_validate(model, train_dataset, valid_dataset, params, savePath=mainPath+'/lossFig.png')\n",
    "else:\n",
    "    model = train_and_validate(model, train_dataset, valid_dataset, params)    \n",
    "elapsed = time.time() - t # running a timer\n",
    "minElapsedStr=str(round(elapsed/60,2))\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"L-Rate: \", params[\"lr\"])\n",
    "print(\"Epochs: \", params[\"epochs\"])\n",
    "print(\"Batches:\", params[\"batch_size\"])\n",
    "print(\"Gamma:  \", params[\"gamma\"])\n",
    "print(\"Alpha:  \", params[\"alpha\"])\n",
    "\n",
    "print('Start time: ' + now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('Elapsed training time (mins): '+ minElapsedStr)\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), mainPath+\"/model.py\")\n",
    "with open(mainPath+\"/params_output.txt\", 'w') as f:\n",
    "    for line in params:\n",
    "        f.write((line+\":\").ljust(15)+str(params[line]))\n",
    "        f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('Path:'.ljust(25) + mainPath)\n",
    "    f.write('\\n')\n",
    "    f.write('Train Duration (min):'.ljust(25) + minElapsedStr)\n",
    "    f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4815ef-31de-4172-a86c-3ed30fcd009b",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c240a3-d4f4-4335-9fb9-02772cc9ec8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getParamsFromText(textfilepath):\n",
    "    \"\"\"Read text file and gets the parameters saved from it\"\"\"\n",
    "    f=open(textfilepath,'r')\n",
    "    lines=f.readlines()\n",
    "    f.close()\n",
    "    params = dict()\n",
    "    for i in range(lines.index(\"\\n\")):\n",
    "        dictTemp=re.compile(\"\\w*(?!>\\s)\").findall(lines[i])[0]\n",
    "        keyTemp=re.compile(\"\\w*\\d*\\.?\\d*$(?!>\\s)\").findall(lines[i][15:])[0]\n",
    "        if keyTemp.isdigit():\n",
    "            keyTemp = int(keyTemp)\n",
    "        elif keyTemp[0].isdigit():\n",
    "            keyTemp = float(keyTemp)\n",
    "        elif keyTemp=='None':\n",
    "            keyTemp = None\n",
    "        params[dictTemp] = keyTemp\n",
    "        \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e81987-519d-4ddd-9b26-a116d0908968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Load\n",
    "# PATH = mainPath + '/model.py'\n",
    "# textFilePath = mainPath + '/params_output.txt'\n",
    "# params=getParamsFromText(textFilePath)\n",
    "# model = UNET_V2(params[\"in_channels\"],params[\"out_channels\"])\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a1c99-172e-40eb-afbf-456c146a4713",
   "metadata": {},
   "source": [
    "## Evaluate and Visualize Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148ad09-3a63-48b7-bc98-63c61e27956c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_test_list = glob.glob(folder_test + '/*.mat')\n",
    "# test_dataset = CustomDataset(folder_data, folder_mask_train, roi_type='roi_full', transform=None)\n",
    "test_dataset = CustomDataset(folder_data, folder_test_list, roi_type=roiFileStrName, transform=None)\n",
    "test_loader = DataLoader(test_dataset,batch_size=1, shuffle=False)\n",
    "\n",
    "# Doing the predictions\n",
    "# predictions = predict(model, params, test_dataset, batch_size=1)\n",
    "predictions,evaluation = evaluateModelAndPredict(model, params, test_dataset, batch_size=1)\n",
    "\n",
    "# output of evaluation metrics\n",
    "for key in evaluation:\n",
    "    print(('Avg ' + key + ':').ljust(25) + str(np.average(evaluation[key])) + ' (' + str((np.average(evaluation[key])*100).round(2)) + '%)')\n",
    "# saving to text file\n",
    "with open(mainPath+\"/params_output.txt\", 'a') as f:\n",
    "    for key in evaluation:\n",
    "        f.write(('Avg ' + key + ':').ljust(25) + str(np.average(evaluation[key])) + ' (' + str((np.average(evaluation[key])*100).round(2)) + '%)')\n",
    "        f.write('\\n')\n",
    "    # printing full output\n",
    "    for key in evaluation:\n",
    "        f.write((key + ':').ljust(25) + str(evaluation[key]))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "display_image_grid_dataset(dataset_obj=test_dataset, numIms=len(test_loader), predicted_masks=predictions)\n",
    "plt.savefig(mainPath+\"/outputFig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1753860-2d85-48e2-b985-cbdec1cd0842",
   "metadata": {},
   "source": [
    "## Visualize Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd04db-c901-4a37-a49c-631633d95dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if params[\"transform\"] is not None:\n",
    "    visualize_augmentations(train_dataset, idx=0, samples=5)\n",
    "    plt.savefig(mainPath+\"/sampleAugmentation.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c063ec-39b2-449d-bc68-0e3e66bd7166",
   "metadata": {},
   "source": [
    "## Probability Map Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ff30-79e0-4246-ba02-5215a0de8e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(\n",
    "#         test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "#     )\n",
    "# model.eval()\n",
    "# predictions = []\n",
    "# with torch.no_grad():\n",
    "#     for images, mask in test_loader:\n",
    "#         images = images.to(params[\"device\"], non_blocking=True)\n",
    "#         output = model(images)\n",
    "#         probabilities = torch.sigmoid(output.squeeze(1))\n",
    "#         predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "#         predicted_masks = predicted_masks.cpu().numpy()\n",
    "#         for predicted_mask in predicted_masks:\n",
    "#             predictions.append(predicted_mask)\n",
    "\n",
    "T_im, T_mask = next(iter(test_loader))\n",
    "model.eval()\n",
    "output = model(T_im.to(params[\"device\"], non_blocking=True))\n",
    "output=output.squeeze(1)\n",
    "print(np.min(output[0,:,:].to('cpu').detach().numpy()),np.max(output[0,:,:].to('cpu').detach().numpy()))\n",
    "probabilities = torch.sigmoid(output)\n",
    "predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "print(np.min(probabilities[0,:,:].to('cpu').detach().numpy()),np.max(probabilities[0,:,:].to('cpu').detach().numpy()))\n",
    "print(np.average(predicted_masks))\n",
    "\n",
    "figure, ax = plt.subplots(1,4, figsize=(15,15))\n",
    "ax[0].imshow(T_im[0,7,:,:].cpu().detach().numpy())\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(T_mask[0,:,:].cpu().detach().numpy())\n",
    "ax[1].set_title('Mask')\n",
    "ax[2].imshow(probabilities[0,:,:].to('cpu').detach().numpy())\n",
    "ax[2].set_title('Probabilities')\n",
    "ax[3].imshow(predicted_masks[0,:,:].cpu().detach().numpy())\n",
    "ax[3].set_title('Predicted Mask');\n",
    "\n",
    "# T_im, T_mask = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf0e69-1e10-4f7b-81ae-ded9627c20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_image_grid_dataset(train_dataset,4)\n",
    "# display_image_grid_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01497e-93a5-4a6d-9eed-aa771340afba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Testing\n",
    "# e = 4 # echo number\n",
    "# # T_im, T_mask = next(iter(train_loader))\n",
    "# # T_im, T_mask = T_im.to(device,dtype=torch.float), T_mask.to(device,dtype=torch.float) # computation on \"floats\" are faster than \"double\" on GPU - hence the cast to float\n",
    "# # print(T_im.shape, T_mask.shape)\n",
    "# # pred = unet(T_im.cuda().float())\n",
    "# # print(pred.shape)\n",
    "\n",
    "# A = pred.cpu().detach().numpy()\n",
    "\n",
    "# f, axarr = plt.subplots(1,4)\n",
    "# plt.figure(figsize=(100,50))\n",
    "# axarr[0].imshow(A[0,0,:,:])\n",
    "# axarr[1].imshow(A[0,1,:,:])\n",
    "# axarr[2].imshow(T_im[0,e,:,:])\n",
    "# axarr[3].imshow(T_mask[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab765d49-8ec6-43d5-b055-f346c620d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # im,mask = next(iter(train_loader))\n",
    "# # im = im[0,:,:,:]\n",
    "# print(np.shape(im))\n",
    "# print('max:', torch.max(im), ',  min:', torch.min(im))\n",
    "\n",
    "# bounds = [torch.min(im), torch.max(im)]\n",
    "# bins=10000\n",
    "# cutoff = 0.95\n",
    "# cutoff1 =0.999\n",
    "# sli = 4\n",
    "# hist = torch.histc(im[sli,:,:].flatten(),bins=bins, min=bounds[0], max=bounds[1])\n",
    "# # normalize histogram to sum to 1\n",
    "# hist = hist.div(hist.sum())\n",
    "# # calculate the bin edges\n",
    "# bin_edges = torch.linspace(bounds[0], bounds[1], steps=bins)\n",
    "# # plotting\n",
    "# plt.plot(bin_edges.cpu().detach().numpy(), hist.cpu().detach().numpy())\n",
    "# plt.title('Original Hist')\n",
    "\n",
    "# # plt.figure()\n",
    "# s = np.cumsum(hist)\n",
    "# plt.figure()\n",
    "# plt.plot(bin_edges.cpu().detach().numpy(), s.cpu().detach().numpy())\n",
    "# plt.title('Cumalitive sum plot (orig)')\n",
    "\n",
    "# binednp = bin_edges.cpu().detach().numpy()\n",
    "# histnp = hist.cpu().detach().numpy()\n",
    "# snp = s.cpu().detach().numpy()\n",
    "# maxval = binednp[snp<cutoff][-1]\n",
    "# print(maxval, np.max(snp))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# nhist = torch.histc(im[sli,:,:].flatten(),bins=bins, min=bounds[0], max=maxval)\n",
    "# nhist = nhist.div(nhist.sum())\n",
    "# plt.plot(bin_edges.cpu().detach().numpy(), nhist)\n",
    "# plt.title('After hist cutoff')\n",
    "\n",
    "# # normalized\n",
    "# image = copy.deepcopy(im)\n",
    "# image = (image - image.mean())/image.std()\n",
    "\n",
    "# # # normalized and histogramed\n",
    "# bounds1 = [torch.min(image), torch.max(image)]\n",
    "# hist1 = torch.histc(image[sli,:,:].flatten(),bins=bins, min=bounds1[0], max=bounds1[1])\n",
    "# # normalize histogram to sum to 1\n",
    "# hist1 = hist1.div(hist1.sum())\n",
    "# # calculate the bin edges\n",
    "# bin_edges1 = torch.linspace(bounds1[0], bounds1[1], steps=bins)\n",
    "# # plotting\n",
    "# plt.figure()\n",
    "# plt.plot(bin_edges1.cpu().detach().numpy(), hist1.cpu().detach().numpy())\n",
    "# plt.title('Normalized Hist')\n",
    "# # calc cut off\n",
    "# binednp1 = bin_edges1.cpu().detach().numpy()\n",
    "# histnp1 = hist1.cpu().detach().numpy()\n",
    "# s1 = np.cumsum(hist1)\n",
    "# snp1 = s1.cpu().detach().numpy()\n",
    "# maxval1 = binednp1[snp1<cutoff][-1]\n",
    "# print(maxval1, np.max(snp1))\n",
    "# # plotting\n",
    "# plt.figure()\n",
    "# nhist1 = torch.histc(image[sli,:,:].flatten(),bins=bins, min=bounds1[0], max=maxval1)\n",
    "# nhist1 = nhist1.div(nhist1.sum())\n",
    "# plt.plot(bin_edges1.cpu().detach().numpy(), nhist1)\n",
    "# plt.title('Normalized and cut-off histogram')\n",
    "\n",
    "\n",
    "# im_cpu = im.cpu().detach().numpy()\n",
    "# ime = copy.deepcopy(im.cpu().detach().numpy())\n",
    "# ime[im_cpu>maxval] = maxval\n",
    "# image_cpu = image.cpu().detach().numpy()\n",
    "# imagee = copy.deepcopy(im.cpu().detach().numpy())\n",
    "# imagee[image_cpu>maxval1] = maxval1\n",
    "# f,ax = plt.subplots(1,4,figsize=[15,15]);\n",
    "# ax[0].imshow(im_cpu[sli,:,:]);\n",
    "# ax[0].title.set_text('original')\n",
    "# ax[1].imshow(ime[sli,:,:]);\n",
    "# ax[1].title.set_text('hist cutoff')\n",
    "# ax[2].imshow(image[sli,:,:])\n",
    "# ax[2].title.set_text('normalized')\n",
    "# ax[3].imshow(imagee[sli,:,:])\n",
    "# ax[3].title.set_text('hist cutoff & normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b5219-32b8-4eaf-8361-bea17254cd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"Testing\"\"\"\n",
    "# all_dataset = CustomDataset(folder_data, glob.glob(folder_mask + '/*.mat'))\n",
    "# all_loader = DataLoader(all_dataset,batch_size=batchsize, shuffle=True)\n",
    "# numtotaldata = len(all_dataset)\n",
    "\n",
    "# train_ds, valid_ds = torch.utils.data.random_split(all_dataset, (int(np.round(numtotaldata*split_train)), int(np.round(numtotaldata*split_valid))))\n",
    "# train_dl = DataLoader(train_ds, batch_size=batchsize, shuffle=True)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=batchsize, shuffle=True)\n",
    "# print(len(train_ds), len(valid_ds))\n",
    "\n",
    "# xb, yb = next(iter(train_dl))\n",
    "# xb.shape, yb.shape\n",
    "\n",
    "# # Testing\n",
    "# # T_im, T_mask = next(iter(train_dataset))\n",
    "# T_im, T_mask = next(iter(train_dl))\n",
    "# T_im, T_mask = T_im.to(device,dtype=torch.float), T_mask.to(device,dtype=torch.float) # computation on \"floats\" are faster than \"double\" on GPU - hence the cast to float\n",
    "# print(T_im.shape, T_mask.shape)\n",
    "# pred = unet(T_im)\n",
    "# print(pred.shape)\n",
    "\n",
    "# ## to clear up cache in GPU?\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg-env",
   "language": "python",
   "name": "seg-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
